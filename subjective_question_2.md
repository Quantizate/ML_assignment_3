# Question 3

### Generating xor dataset
Input features ($x1$ and $x2$) are generated by using Normal Distribution with zero mean and 250 standard deviation. The outputs are generated by multiplying $x1$ and $x2$ and assign class 1 if product is positive and class 0 if product is negative.

### (a) MLP
<img src = "./images/Plot_MLP.PNG" width="700"/>

The MLP has two hidden layes with 8 and 4 neurons respectively. The architecture and training procedure is kept same for all the parts. It learned the surface in under 1000 epochs. We can see the clear desicion boundary with a 98% test accuracy. We get such good results because a MLP can learn difficult decision boundaries (hyperbolic in this case) when appropriate hidden layers and neurons are provided.

### (b) MLP with L1 regularisation
<img src = "./images/MLP_L1.PNG" width="700"/>

We got 98.5% test accuracy. The decision boundary is not as clear as before. It is wider than a unregularised MLP model.

### (b) MLP with L2 regularisation
<img src = "./images/MLP_L2.PNG" width="700"/>

We got 98% test accuracy. The decision boundary is not as clear as before. It is wider than L1 regularised MLP.

### (d) Logistic Regression with extra features
#### Model 1 with only $x1$ and $x2$
<img src = "./images/LR_1.PNG" width="700"/>

We got 56% test accuracy. The decision boundary is not visible as the accuracy is quite less. Complete meshgrid is identified as a single class (class 0). It could not learn any disicion boundary because the feature were too simple (linear) and the model exhibit high bias.

#### Model 2 with only x1 $\cdot$ x2
<img src = "./images/LR_2.PNG" width="700"/>

We got 100% test accuracy. The decision boundary is very clear as the model is perfectly accurate. All the data points are identified correct. This happened because the datapoints exhibit a hyperbolic shape and x1 $\cdot$ x2 is a hyperbolic function indeed. Thus showing great results.

#### Model 3 with $x1$, $x2$, $x1^{2}$ and $x2^{2}$
<img src = "./images/LR_3.PNG" width="700"/>

The accuracy is 55%. This time some decision boundary is seen because it had more complex non linear features to learn unlike in model 1. There is a lot of misclassification and the model exhibits high bias.
